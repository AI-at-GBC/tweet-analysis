{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project"
      ],
      "metadata": {
        "id": "i6xjEFBJQ73X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "WB--FK2SQ98v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmb-1j_37FcV",
        "outputId": "61e89b8a-3866-4870-c012-51df0314c92d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: config in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install 'tweepy==4.4.0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klwzt_N__SMM",
        "outputId": "880f05c5-bd6a-4867-8da3-40a51a0d4f5a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy==4.4.0 in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.4.0) (1.3.0)\n",
            "Requirement already satisfied: requests<3,>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.4.0) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.11.1->tweepy==4.4.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.11.1->tweepy==4.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.11.1->tweepy==4.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.11.1->tweepy==4.4.0) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib<2,>=1.0.0->tweepy==4.4.0) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import keras\n",
        "import keras.preprocessing.text as kpt\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing, model_selection\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "\n",
        "import tweepy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fiIiewfyjXj",
        "outputId": "49ac500f-8197-425e-a3b8-f9f1c94e853b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_PATH = '/content/drive/MyDrive/ML/GBC/DL1/Project/'"
      ],
      "metadata": {
        "id": "P-Z5G9OFUghr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Tweets"
      ],
      "metadata": {
        "id": "UZpFsMhEQ6vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import connection info\n",
        "twitter_api_info = pd.read_csv(FILE_PATH + 'CONNECTION_INFO.csv')\n",
        "twitter_api_info = twitter_api_info.set_index('key')"
      ],
      "metadata": {
        "id": "6qHaWhbzykAO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = twitter_api_info.loc['API_KEY']['value']\n",
        "API_KEY_SECRET = twitter_api_info.loc['API_KEY_SECRET']['value']\n",
        "BEARER_TOKEN = twitter_api_info.loc['BEARER_TOKEN']['value']\n",
        "ACCESS_TOKEN = twitter_api_info.loc['ACCESS_TOKEN']['value']\n",
        "ACCESS_TOKEN_SECRET = twitter_api_info.loc['ACCESS_TOKEN_SECRET']['value']"
      ],
      "metadata": {
        "id": "m1-CXzS17HQP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to grab tweets\n",
        "\n",
        "def getClient():\n",
        "    client = tweepy.Client(\n",
        "        bearer_token=BEARER_TOKEN,\n",
        "        consumer_key=API_KEY,\n",
        "        consumer_secret=API_KEY_SECRET,\n",
        "        access_token=ACCESS_TOKEN,\n",
        "        access_token_secret=ACCESS_TOKEN_SECRET\n",
        "    )\n",
        "    return client\n",
        "\n",
        "def searchTweets(client, query, max_results):\n",
        "    tweets = client.search_recent_tweets(query=query, max_results=max_results)\n",
        "\n",
        "    tweet_data =  tweets.data\n",
        "    results = []\n",
        "\n",
        "    if not tweet_data is None and len(tweet_data) > 0:\n",
        "        for tweet in tweet_data:\n",
        "            obj = {}\n",
        "            obj['id'] = tweet.id\n",
        "            obj['text'] = tweet.text\n",
        "            results.append(obj)\n",
        "\n",
        "    return results\n",
        "\n",
        "def returnTweets(query):\n",
        "    query = '{} lang:en -is:retweet'.format(query)\n",
        "\n",
        "    client = getClient()\n",
        "    tweets = searchTweets(client, query, 50)\n",
        "\n",
        "    objs = []\n",
        "\n",
        "    if len(tweets) > 0:\n",
        "        for tweet in tweets:\n",
        "            obj = {}\n",
        "            obj['text'] = tweet['text']\n",
        "            objs.append(obj)\n",
        "\n",
        "    return(objs)"
      ],
      "metadata": {
        "id": "tQvIZLPG7Il-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX9TKaTQ6iCs",
        "outputId": "de0e1239-917d-41b1-8791-c800d80f519e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 1485819882667397124, 'text': 'RT @BitcoinMagazine: \"The safest and most profitable moment to buy is when the price is down.\" â€“ El Salvador president on #Bitcoin https://â€¦'}\n",
            "{'id': 1485819882067349508, 'text': 'Trending on #LunarCrush:\\n\\n\"Investidores aproveitam queda do Bitcoin e investem US$ 14 milhÃµes em fundos de criptomoedas\" via @infomoney\\n\\nTop coin mentions\\n$btc\\n\\nhttps://t.co/wuzaPWGHIE'}\n",
            "{'id': 1485819880116998146, 'text': 'RT @freecashcom: $100.00 #Bitcoin Giveaway\\n\\n- Retweet\\n- Follow us\\n\\nRolling in 24 hours ðŸ”¥'}\n",
            "{'id': 1485819878812819456, 'text': 'I am in the running to win the Metaverse Lab Airdrop with the Prizepool of 20,000 $MVP ,Check it out #Giveaway #Airdrop #BNB #Solana #Bitcoin https://t.co/YClXFm7hGU'}\n",
            "{'id': 1485819876065386496, 'text': '#Bitcoin es y serÃ¡ siempre un resguardo de valor. Siempre serÃ¡ mejor tener un registro mundial descentralizado que confiar en instituciones corrompibles. ðŸ‘€ https://t.co/mcFabG6qDR'}\n",
            "{'id': 1485819875834855424, 'text': 'RT @WebcoinCapital: ðŸ”¥Metaverse Lab Airdrop is now live ðŸš€\\n\\nTo celebrate ðŸ¥‚ Launch of @LabMetaverse , we will #Giveaway 10,000 $MVP + MVP NFTâ€¦'}\n",
            "{'id': 1485819875474161667, 'text': 'RT @AirdropDet: ðŸ” New #Airdrop: AME Chain\\n\\nðŸ’²Reward: Up to 500 AME + 200K AME referral pool\\n\\nðŸ”´ Start the airdrop bot https://t.co/GH64jlLXPlâ€¦'}\n",
            "{'id': 1485819875092307969, 'text': 'â€œDaaaaamn shawty imma call you bitcoin the way you dipâ€\\n\\n#Bitcoin #bitcoinprice #nftcomedy #nftsrt #NFTCommunity #nftphotographer'}\n",
            "{'id': 1485819875088281604, 'text': 'RT @airdropinspect: New airdrop: ADADAO X Cardence (AUSD)\\nTotal Reward: $20,000 worth of AUSD\\nRate: â­ï¸â­ï¸â­ï¸â­ï¸\\nWinners: 2,000 Random\\nDistribuâ€¦'}\n",
            "{'id': 1485819874832269321, 'text': 'RT @rockerfellerxau: sol taraf 2021 mayÄ±sta baÅŸlayan temmuzda dip yapan dÃ¼ÅŸÃ¼ÅŸ!\\n\\nsaÄŸ taraf kasÄ±mda tepe yapan ve ocakta dip yapÄ±p yÃ¼kseliÅŸeâ€¦'}\n"
          ]
        }
      ],
      "source": [
        "tweets = searchTweets(getClient(), 'bitcoin', 10)\n",
        "# tweets\n",
        "for x in tweets:\n",
        "    print(x)\n",
        "\n",
        "# query = 'canada'\n",
        "# returnTweets(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis"
      ],
      "metadata": {
        "id": "gujgyM1TQ225"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Stop words and add capital I\n",
        "stopwords = stopwords.words('english')\n",
        "new_words=['I']\n",
        "for i in new_words:\n",
        "    stopwords.append(i)"
      ],
      "metadata": {
        "id": "923RjI0BQ5o0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data set\n",
        "# You may need pandas >1.3.0 for \"on_bad_lines\" to work\n",
        "data = pd.read_csv(FILE_PATH + '/sentiment_analysis_dataset.csv', sep=',', encoding='latin-1', on_bad_lines='skip')"
      ],
      "metadata": {
        "id": "MyWsuJh6TG_e"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all the stop words inn the dataset\n",
        "data['X_Clean'] = data['SentimentText'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
        "\n",
        "# Define X & Y\n",
        "Y = data.Sentiment\n",
        "X = data.X_Clean\n",
        "\n",
        "\n",
        "X = X.iloc[:10000]\n",
        "Y = Y.iloc[:10000]\n"
      ],
      "metadata": {
        "id": "lfBVU21kVIG6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only work with the 3000 most popular words found in our dataset\n",
        "max_words = 2000\n",
        "\n",
        "# create a new Tokenizer\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "\n",
        "# feed our tweets to the Tokenizer\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "# Tokenizers come with a convenient list of words and IDs\n",
        "dictionary = tokenizer.word_index\n",
        "\n",
        "\n",
        "with open('dictionary.json', 'w') as dictionary_file:\n",
        "    json.dump(dictionary, dictionary_file)\n",
        "    \n",
        "\n",
        "def convert_text_to_index_array(text):\n",
        "    #  text_to_word_sequence makes all texts the same length -- in this case, the length\n",
        "    # of the longest text in the set.\n",
        "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
        "\n",
        "\n",
        "\n",
        "allIndices = []\n",
        "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
        "for text in X:\n",
        "    wordIndices = convert_text_to_index_array(text)\n",
        "    allIndices.append(wordIndices)\n",
        "    \n",
        "    \n",
        "# now we have a list of all tweets converted to index arrays.\n",
        "# cast as an array for future usage.\n",
        "allIndices = np.asarray(allIndices,dtype=object)  \n",
        "    \n",
        "# create one-hot matrices out of the indexed tweets\n",
        "train_x = tokenizer.sequences_to_matrix(allIndices, mode='binary')\n",
        "# treat the labels as categories\n",
        "train_y = tf.keras.utils.to_categorical(\n",
        "    Y, num_classes=2, dtype='float32')\n",
        "\n",
        "train_x, test_x, train_y, test_y = model_selection.train_test_split(train_x,train_y,test_size = 0.2, random_state = 0)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
        "# model.add(Dropout(0.1))\n",
        "model.add(Dense(256, activation='sigmoid'))\n",
        "# model.add(Dropout(0.1))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "    \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy'])   \n",
        "    \n",
        "\n",
        "model.fit(train_x, train_y,\n",
        "  batch_size=80,\n",
        "  epochs=5,\n",
        "  validation_split=0.1,\n",
        "  shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW2ZQRyPRllk",
        "outputId": "ca24e2f4-5b67-4ce5-c8bd-268fe338dfec"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "90/90 [==============================] - 2s 5ms/step - loss: 0.5916 - accuracy: 0.6760 - val_loss: 0.5293 - val_accuracy: 0.7350\n",
            "Epoch 2/5\n",
            "90/90 [==============================] - 0s 3ms/step - loss: 0.3669 - accuracy: 0.8331 - val_loss: 0.5255 - val_accuracy: 0.7563\n",
            "Epoch 3/5\n",
            "90/90 [==============================] - 0s 3ms/step - loss: 0.2348 - accuracy: 0.9076 - val_loss: 0.6329 - val_accuracy: 0.7412\n",
            "Epoch 4/5\n",
            "90/90 [==============================] - 0s 3ms/step - loss: 0.1422 - accuracy: 0.9475 - val_loss: 0.7592 - val_accuracy: 0.7450\n",
            "Epoch 5/5\n",
            "90/90 [==============================] - 0s 3ms/step - loss: 0.0853 - accuracy: 0.9692 - val_loss: 0.8538 - val_accuracy: 0.7475\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f63a9a74090>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}
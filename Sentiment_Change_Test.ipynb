{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Change Test.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmxhm5AMxAFg",
        "outputId": "1856629c-c752-47b8-844e-72a8c6a12296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.8.1\n",
            "  Downloading torchtext-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 3.8 MB/s \n",
            "\u001b[?25hCollecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 16 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->torchtext==0.8.1) (3.10.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (2.10)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1 torchtext-0.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torchtext==0.8.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 -m spacy download en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4MkPSm2xDC7",
        "outputId": "9a0c1ba2-798d-4088-fd7b-ead92c22c5f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 501 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install msgpack==0.5.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGoxHKZYxE4r",
        "outputId": "e54ba21a-0bdf-425b-c5ce-3151ac193e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting msgpack==0.5.6\n",
            "  Downloading msgpack-0.5.6.tar.gz (138 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▍                             | 10 kB 33.1 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 20 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 30 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 40 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 51 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 81 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 102 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 112 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 122 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 133 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 138 kB 4.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: msgpack\n",
            "  Building wheel for msgpack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for msgpack: filename=msgpack-0.5.6-cp37-cp37m-linux_x86_64.whl size=298920 sha256=8a23c9bf42c4c2b179d105b9048dbd919e6bb037659bfbe808739c26f60bc75e\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/92/5b/708850ed8872c249ce8ace306b542e1099428491e6b613297b\n",
            "Successfully built msgpack\n",
            "Installing collected packages: msgpack\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.0.3\n",
            "    Uninstalling msgpack-1.0.3:\n",
            "      Successfully uninstalled msgpack-1.0.3\n",
            "Successfully installed msgpack-0.5.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import random\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "\n",
        "TEXT = data.Field(tokenize='spacy')\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "train, valid = train.split(random_state=random.seed(SEED))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUmBbpsQxKhQ",
        "outputId": "e5d3252a-49e4-478c-99fc-a1098592162e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 12.0MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B3hkwkBxMPO",
        "outputId": "4b489268-0d6c-407b-ced2-2f976442f090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "189"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(train) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZw8Ah57yK1J",
        "outputId": "e7034f8a-e913-4dc3-fcc8-ecc101bb2266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:43, 5.29MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:16<00:00, 24495.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_w=TEXT.vocab.itos[9205]\n",
        "test_w2=TEXT.vocab.itos[9206]\n",
        "\n",
        "print(test_w)\n",
        "print(test_w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV5jzFRAEo6l",
        "outputId": "07b72bef-68e4-4345-e944-69b0a0cb425a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hats\n",
            "healing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import cosine_similarity\n",
        "test_v=TEXT.vocab.vectors[9205].unsqueeze(0)\n",
        "test_v2=TEXT.vocab.vectors[9206]\n",
        "\n",
        "cosine_similarity(test_v,TEXT.vocab.vectors,dim=1).sort()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jA4uOTZeGXXV",
        "outputId": "a53aa49d-8a03-48af-9bee-0445cb4f8039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.sort(values=tensor([-0.2809, -0.2443, -0.2430,  ...,  0.7779,  0.7918,  1.0000]), indices=tensor([24894, 10858, 21976,  ...,  9155,  9277,  9205]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(TEXT.vocab.itos[538])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTxRnIL-GZmU",
        "outputId": "e27d6f7d-db64-4d5f-b82d-396cc131e588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "genre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train, valid, test), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    sort_key=lambda x: len(x.text), \n",
        "    repeat=False,\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6W7s_PYGas9",
        "outputId": "82769ec7-1bd3-43ff-c91d-76b69f5d2b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Model"
      ],
      "metadata": {
        "id": "eJtzzvZdGcvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0],embedding_dim))\n",
        "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1],embedding_dim))\n",
        "        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2],embedding_dim))\n",
        "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [sent len, batch size]\n",
        "        \n",
        "        x = x.permute(1, 0)\n",
        "                \n",
        "        #x = [batch size, sent len]\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        \n",
        "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
        "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
        "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
        "            \n",
        "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
        "        \n",
        "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
        "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "            \n",
        "        return self.fc(cat)"
      ],
      "metadata": {
        "id": "rAMJpeS7GeJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs,embedding_dim)) for fs in filter_sizes])\n",
        "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [sent len, batch size]\n",
        "        \n",
        "        x = x.permute(1, 0)\n",
        "                \n",
        "        #x = [batch size, sent len]\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        \n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "            \n",
        "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
        "        \n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "            \n",
        "        return self.fc(cat)"
      ],
      "metadata": {
        "id": "y_gVtNnKGi7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate our CNN class\n",
        "\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3,4,5]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)"
      ],
      "metadata": {
        "id": "d1e5MO8jGkWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da0jRocYGnOa",
        "outputId": "0d30f7c1-33ed-457e-da2b-b56eef8f186f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-0.7949, -0.3553, -0.7039,  ...,  1.2660,  0.3886,  0.6810],\n",
              "        [-0.3033,  1.0977, -0.4089,  ...,  1.5036,  0.1848,  0.4442],\n",
              "        [-0.6585,  0.7627, -0.4833,  ..., -0.4813, -0.3814,  0.0460]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "TmHjLnLUGoYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y.float()).float() #convert into float for division \n",
        "    acc = correct.sum()/len(correct)\n",
        "    return acc"
      ],
      "metadata": {
        "id": "cjEl3EzJGqdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label.float())\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "XJ-r9lcSGvz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label.float())\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "hOhPGHU1GxSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    train_loss, train_acc = train_model(model, train_iterator, optimizer, criterion)\n",
        "    #valid_loss, valid_acc = evaluate_model(model, valid_iterator, criterion)\n",
        "    valid_loss=0\n",
        "    valid_acc=0\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNTzyrKFGyYv",
        "outputId": "49a01338-bff2-4c78-bb2e-81725da3ee8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Train Loss: 0.500, Train Acc: 74.31%, Val. Loss: 0.000, Val. Acc: 0.00%\n",
            "Epoch: 02, Train Loss: 0.307, Train Acc: 87.22%, Val. Loss: 0.000, Val. Acc: 0.00%\n",
            "Epoch: 03, Train Loss: 0.218, Train Acc: 91.59%, Val. Loss: 0.000, Val. Acc: 0.00%\n",
            "Epoch: 04, Train Loss: 0.143, Train Acc: 94.83%, Val. Loss: 0.000, Val. Acc: 0.00%\n",
            "Epoch: 05, Train Loss: 0.092, Train Acc: 96.98%, Val. Loss: 0.000, Val. Acc: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate_model(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_Xk_ymOHDMP",
        "outputId": "32b88ac7-e0a9-436f-9a7f-69fba345aeef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.305, Test Acc: 88.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some auxiliary functions in order to make a color pallete. \n",
        "# Couresy of https://www.oreilly.com/library/view/python-cookbook/0596001673/ch09s11.html\n",
        "\n",
        "import math\n",
        "\n",
        "def floatRgb(mag):\n",
        "    \"\"\" Return a tuple of floats between 0 and 1 for R, G, and B. \"\"\"\n",
        "    \n",
        "    #blue  = min((max((4*(0.75-x), 0.)), 1.))\n",
        "    #red   = min((max((4*(x-0.25), 0.)), 1.))\n",
        "    #green = min((max((4*math.fabs(x-0.5)-1., 0.)), 1.))\n",
        "    red=0\n",
        "    blue=0\n",
        "    green =0\n",
        "    \n",
        "    if mag>0:\n",
        "      blue=min(1,mag/3)\n",
        "    else:\n",
        "      if mag<0:\n",
        "        red=min(1,-mag/3)\n",
        "        \n",
        "        \n",
        "     \n",
        "    \n",
        "    return red, green, blue\n",
        "  \n",
        "def rgb(mag):\n",
        "    \"\"\" Return a tuple of integers, as used in AWT/Java plots. \"\"\"\n",
        "    red, green, blue = floatRgb(mag)\n",
        "    return int(red*255), int(green*255), int(blue*255)\n",
        "\n",
        "def strRgb(mag):\n",
        "    \"\"\" Return a hex string, as used in Tk plots. \"\"\"\n",
        "    return \"#%02x%02x%02x\" % rgb(mag)"
      ],
      "metadata": {
        "id": "SDvPfEdYHECL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "from IPython.core.display import display,HTML\n",
        "from torch.nn.functional import cosine_similarity\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "def predict_sentiment(sentence, explain_scores=True,explain_relative_to=1):\n",
        "  \n",
        "    tokenized_sentence = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed_sentence = [TEXT.vocab.stoi[t] for t in tokenized_sentence]\n",
        "    tensor = torch.LongTensor(indexed_sentence).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    \n",
        "    original_input_embedding,input_grad=get_input_gradients(indexed_sentence,prediction,explain_relative_to)\n",
        "    \n",
        "    explanation=get_prediction_explanation(tokenized_sentence,original_input_embedding,input_grad,explain_scores)\n",
        "      \n",
        "    return {'tokenized_sentence': tokenized_sentence,'prediction': prediction.item(),'explanation': explanation }\n",
        "\n",
        "def get_input_gradients(original_sentence,prediction,in_relation_to):\n",
        "    gradient_truth=torch.Tensor([in_relation_to]).unsqueeze(0)\n",
        "    if torch.cuda.is_available():\n",
        "      gradient_truth=gradient_truth.cuda()\n",
        "    \n",
        "    loss=criterion(prediction,gradient_truth)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    \n",
        "    input_grad=torch.Tensor(len(original_sentence),model.embedding.weight.size(1))\n",
        "    original_input_embedding=torch.Tensor(len(original_sentence),model.embedding.weight.size(1))\n",
        "    \n",
        "    for i in range(0,len(original_sentence)):\n",
        "      original_input_embedding[i]=model.embedding.weight[original_sentence[i]]\n",
        "      input_grad[i]=model.embedding.weight.grad[original_sentence[i]]\n",
        "    \n",
        "    return original_input_embedding,input_grad\n",
        "    \n",
        "    \n",
        "\n",
        "def get_input_scores(input,input_embedding,input_grad):\n",
        "  \n",
        "  \n",
        "  # Take a SGD step using grads\n",
        "  \n",
        "  input_after_step=input_embedding-input_grad\n",
        "  after_grad_norms = torch.norm(input_after_step, 2, 1)\n",
        "  before_grad_norms = torch.norm(input_embedding, 2, 1)\n",
        "  variation = after_grad_norms-before_grad_norms\n",
        " \n",
        "  standard_deviation=torch.std(variation)\n",
        "  mean=torch.mean(variation)\n",
        "  z_score=(variation-mean)/standard_deviation\n",
        " \n",
        "  return z_score\n",
        "\n",
        "def old_get_input_scores(input,input_embedding,input_grad):\n",
        "  \n",
        "  grad_norms=torch.norm(input_grad,2,1)\n",
        "  \n",
        "  return grad_norms/torch.max(grad_norms)\n",
        "\n",
        "  \n",
        "def get_prediction_explanation(input,input_embedding,input_grad, explain_scores):\n",
        " \n",
        "  \n",
        "  input_word_scores=get_input_scores(input,input_embedding,input_grad)\n",
        "   \n",
        "  explanation=\"\"\n",
        "  for i in range(0,len(input)):\n",
        "    token=input[i]\n",
        "    token_color=strRgb(input_word_scores[i])\n",
        "    if explain_scores:\n",
        "      str_token=\"%s (%.3f)\"%(token,input_word_scores[i])\n",
        "    else:\n",
        "      str_token=token\n",
        "    \n",
        "    explanation=explanation+'<font color=\"'+token_color+'\">'+str_token+'&nbsp;</font>'\n",
        "    if i>0 and i%20==0:\n",
        "      explanation=explanation+\"<br/>\"\n",
        "    \n",
        "  return {'word_scores': input_word_scores,'input_gradient': input_grad,'textual_explanation':explanation }\n",
        "\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "FUPgvmG4HG8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import cosine_similarity\n",
        "\n",
        "def get_projected_words(word,word_gradient,num_words=1):\n",
        "  \n",
        "  word_index=TEXT.vocab.stoi[word]\n",
        "  word_embedding=TEXT.vocab.vectors[word_index]\n",
        "  learning_rate=1\n",
        "  i=0\n",
        "  result=[]\n",
        "  \n",
        "  while i<100000:\n",
        "    try: \n",
        "      word_embedding=word_embedding-learning_rate*word_gradient\n",
        "    except:\n",
        "      # We can have a float overflow here if this process gets out of control\n",
        "      return result\n",
        "    similarity_value,similarity_index=cosine_similarity(word_embedding.unsqueeze(0),TEXT.vocab.vectors,dim=1).sort(descending=True)\n",
        "    if similarity_index[0]!=word_index:\n",
        "      if  similarity_value[0]<0.5:\n",
        "        break\n",
        "      \n",
        "      result.append({'word':TEXT.vocab.itos[similarity_index[0]],'similarity': similarity_value[0]})\n",
        "      word_index=similarity_index[0]\n",
        "      learning_rate=1\n",
        "      if len(result)>=num_words:\n",
        "        break\n",
        "      \n",
        "    i=i+1\n",
        "    learning_rate=learning_rate*1.1\n",
        "      \n",
        "  return result\n",
        "\n",
        "  \n",
        "def get_projected_sentence_word(prediction,word):\n",
        "  \n",
        "  sentence=prediction['tokenized_sentence']\n",
        "  word_index_in_sentence=[i for i in range(0,len(sentence)) if sentence[i]==word][0]\n",
        "  word_gradient=prediction['explanation']['input_gradient'][word_index_in_sentence]\n",
        "  \n",
        "  \n",
        "  return get_projected_words(word,word_gradient,1)\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "r9-UK8QzHHyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=predict_sentiment(\"This is a ridiculous movie and you should never see it.\")\n",
        "print(prediction['prediction'])\n",
        "display(HTML(prediction['explanation']['textual_explanation']))\n",
        "print(get_projected_sentence_word(prediction,'ridiculous'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "FLDTqVb5HI6h",
        "outputId": "b17371a7-7df6-460a-c62b-74ab32f4c6d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.23816709220409393\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<font color=\"#000055\">This (1.010)&nbsp;</font><font color=\"#00001c\">is (0.331)&nbsp;</font><font color=\"#000033\">a (0.605)&nbsp;</font><font color=\"#fe0000\">ridiculous (-2.999)&nbsp;</font><font color=\"#000002\">movie (0.035)&nbsp;</font><font color=\"#000035\">and (0.624)&nbsp;</font><font color=\"#00001d\">you (0.345)&nbsp;</font><font color=\"#000000\">should (-0.009)&nbsp;</font><font color=\"#000001\">never (0.014)&nbsp;</font><font color=\"#000001\">see (0.014)&nbsp;</font><font color=\"#000001\">it (0.014)&nbsp;</font><font color=\"#000001\">. (0.014)&nbsp;</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'word': 'interesting', 'similarity': tensor(0.7005)}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=predict_sentiment(\"Bitcoin sucks ass, divest right now.\")\n",
        "print(prediction['prediction'])\n",
        "display(HTML(prediction['explanation']['textual_explanation']))\n",
        "# print(get_projected_sentence_word(prediction,'ridiculous'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xiO5LdI9g5d1",
        "outputId": "006b49b3-3d3f-4474-e23c-f8b5eec98e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.19795987010002136\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<font color=\"#00006e\">Bitcoin (1.303)&nbsp;</font><font color=\"#9d0000\">sucks (-1.854)&nbsp;</font><font color=\"#130000\">ass (-0.231)&nbsp;</font><font color=\"#000004\">, (0.055)&nbsp;</font><font color=\"#00006e\">divest (1.303)&nbsp;</font><font color=\"#100000\">right (-0.191)&nbsp;</font><font color=\"#100000\">now (-0.193)&nbsp;</font><font color=\"#100000\">. (-0.193)&nbsp;</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YMDyA8VNL-3"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72ShFB-4NFaU",
    "outputId": "a09eee88-5661-464a-f0c8-1db61314185c"
   },
   "outputs": [],
   "source": [
    "# This may require a runtime restart in order to work\n",
    "!pip install 'pandas==1.3.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2CMXroCN0D35",
    "outputId": "3d62460d-6275-4c1c-fd78-7e0956ab5843"
   },
   "outputs": [],
   "source": [
    "!pip install config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twij53zRzrrU",
    "outputId": "cc3f67ea-40b6-44b7-afd9-658dc425d7e5"
   },
   "outputs": [],
   "source": [
    "!pip install 'tweepy==4.4.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "row3DN8VXxRB",
    "outputId": "e613b5b0-20c0-4f66-b4e3-b38f5c4a1811"
   },
   "outputs": [],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgiRPiqx36lp",
    "outputId": "6d71ad03-ec72-4583-8d3d-f32ffe5e3aec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Standard Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLTKPackages\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Nearal Network packages\n",
    "#import keras\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing, model_selection\n",
    "#from keras.models import Sequential, load_model \n",
    "#from keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Sklearn pacckages \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "PN_Zd9tL1dXa",
    "outputId": "9c3c267f-fc56-4c2f-9307-b594cfd1bcd7"
   },
   "outputs": [],
   "source": [
    "#Loading the dataset for mac users\n",
    "FILE_PATH = '/content/drive/MyDrive/ML/GBC/DL1/Project/'\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Load the data set\n",
    "data = pd.read_csv(\n",
    "    FILE_PATH + 'sentiment_analysis_dataset.csv',\n",
    "    sep=',',\n",
    "    on_bad_lines='skip',\n",
    "    encoding='latin-1'\n",
    "    )\n",
    "\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1578612, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ï»¿ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ï»¿ItemID  Sentiment SentimentSource  \\\n",
       "0          1          0    Sentiment140   \n",
       "1          2          0    Sentiment140   \n",
       "2          3          1    Sentiment140   \n",
       "3          4          0    Sentiment140   \n",
       "4          5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset for PC users\n",
    "data = pd.read_csv('sentiment analysis dataset.csv', sep=',', on_bad_lines='skip', encoding='latin-1')\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L2R1emkARard",
    "outputId": "e423e974-d78c-4e83-f312-9d9ab7ef1515"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ï»¿ItemID          0\n",
       "Sentiment          0\n",
       "SentimentSource    0\n",
       "SentimentText      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null data\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "E3so0G5USutZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105241, 3)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Item ID column is not useful for us, drop it\n",
    "# We also tested using 1/3 of the dataset and ran out of memory, 1/4 seems to be our limit\n",
    "data = data.drop(['ï»¿ItemID'], axis=1)\n",
    "data = data.sample(frac=1/15).reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>@joelMadden Im up too. I can't sleeeeeeeep! An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>work at 8am..no fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>@PursuitBrooke Are you more of a Summer person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>wants to smoke a cig but doesn't have a lighter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>what pass?? there are lots of twits but we don...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment SentimentSource  \\\n",
       "0          0    Sentiment140   \n",
       "1          0    Sentiment140   \n",
       "2          1    Sentiment140   \n",
       "3          0    Sentiment140   \n",
       "4          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0  @joelMadden Im up too. I can't sleeeeeeeep! An...  \n",
       "1                               work at 8am..no fun   \n",
       "2  @PursuitBrooke Are you more of a Summer person...  \n",
       "3  wants to smoke a cig but doesn't have a lighter.   \n",
       "4  what pass?? there are lots of twits but we don...  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned text started\n",
      "Wall time: 1min 53s\n",
      "cleaned text completed\n",
      "Wall time: 0 ns\n",
      "tfidf vectorizer started\n",
      "Wall time: 14 ms\n",
      "tfidf vectorizer completed\n",
      "Wall time: 1.02 ms\n",
      "column names started\n",
      "Wall time: 0 ns\n",
      "column names completed\n"
     ]
    }
   ],
   "source": [
    "# # Clean text\n",
    "# import re\n",
    "# from string import punctuation\n",
    "# from collections import Counter\n",
    "\n",
    "# from spellchecker import SpellChecker\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# def clean_text_for_tfidf_vectorizer(text):\n",
    "#     spell = SpellChecker()\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tk = TweetTokenizer()\n",
    "\n",
    "#     stopword_list = stopwords.words('english')\n",
    "#     new_stop_words=['i', 'im', 'http', 'ive', 'rt']\n",
    "#     for i in new_stop_words:\n",
    "#         stopword_list.append(i)\n",
    "\n",
    "#     cleaned_text = []\n",
    "#     punctuation_counts = []\n",
    "\n",
    "#     for sentence in text:\n",
    "#         cleaned_words = []\n",
    "\n",
    "#         punctuation_count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "#         punctuation_counts.append(punctuation_count(sentence,set(punctuation)))\n",
    "\n",
    "#         for word in tk.tokenize(sentence):\n",
    "#             # Spell check\n",
    "#             word = spell.correction(word.lower())\n",
    "\n",
    "#             # Remove stop words\n",
    "#             if word in stopword_list:\n",
    "#               continue\n",
    "\n",
    "#             # Remove numbers and punctuation\n",
    "#             word = re.sub('[^a-zA-Z]+', '', word)\n",
    "\n",
    "#             if (word == ''):\n",
    "#               continue\n",
    "\n",
    "#             # Lemmatize\n",
    "#             word = lemmatizer.lemmatize(word)\n",
    "\n",
    "#             cleaned_words.append(word)\n",
    "\n",
    "#         cleaned_text.append(' '.join(cleaned_words))\n",
    "      \n",
    "#     return cleaned_text, punctuation_counts\n",
    "\n",
    "# # Checking the run time of the script on a small subset of the dataset\n",
    "# text = data['SentimentText']\n",
    "# text = text.loc[1:500]\n",
    "\n",
    "# print('cleaned text started')\n",
    "# cleaned_text, punctuation_counts = %time clean_text_for_tfidf_vectorizer(text)\n",
    "# print('cleaned text completed')\n",
    " \n",
    "# tfidf_vectorizer = %time TfidfVectorizer(ngram_range=(1,3))\n",
    "\n",
    "# print('tfidf vectorizer started')\n",
    "# tfidf_vect_w_mat = %time tfidf_vectorizer.fit_transform(cleaned_text)\n",
    "# print('tfidf vectorizer completed')\n",
    "\n",
    "# tfidf_feature_names = %time tfidf_vectorizer.get_feature_names_out()\n",
    "# tfidf_df = pd.DataFrame (tfidf_vect_w_mat.todense())\n",
    "\n",
    "# print('column names started')\n",
    "# %time tfidf_df.columns = tfidf_feature_names # This may be a very expensive operation, consider commenting out\n",
    "# print('column names completed')\n",
    "\n",
    "# tfidf_df['punctuation_count'] = punctuation_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried this function to clean the dataset and benchmarked it's performance.  Only 500 tweets took almost 2 minutes, checked to see if we can reduce the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After various testing, the expensive function is the spell checker.  We will be creating 2 versions of the database, one with spell checker and one without so we can move forward with model building\n",
    "while the spell checker compiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Function\n",
      "Wall time: 38min 35s\n",
      "Ending Function\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "stopwordlist = stopwords.words('english')\n",
    "new_stop_words=['i', 'im', 'http', 'ive', 'rt']\n",
    "for i in new_stop_words:\n",
    "    stopwordlist.append(i)\n",
    "\n",
    "\n",
    "text = data['SentimentText']\n",
    "sentiment = data['Sentiment']\n",
    "\n",
    "\n",
    "def preprocess(textdata):\n",
    "    processedText = []\n",
    "    punctuation_counts = []\n",
    "\n",
    "    # Create Lemmatizer and Stemmer.\n",
    "    wordstem = PorterStemmer()\n",
    "    spell = SpellChecker()\n",
    "    tk = TweetTokenizer()\n",
    "    \n",
    "    for tweet in textdata:\n",
    "        tweet = re.sub(r'@([A-Za-z0-9_]+)', '', tweet)\n",
    "        punctuation_count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "        punctuation_counts.append(punctuation_count(tweet,set(punctuation)))\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        # Regex\n",
    "        tweet = re.sub('[^a-zA-Z]+', ' ', tweet)\n",
    "        sequencePattern   = r\"(.)\\1\\1+\"\n",
    "        seqReplacePattern = r\"\\1\\1\"\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "        \n",
    "        tweetwords = ''\n",
    "        for word in tk.tokenize(tweet):\n",
    "            # Checking if the word is a stopword.\n",
    "            if word not in stopwordlist:\n",
    "                # Spell check the word\n",
    "                word = spell.correction(word)\n",
    "                # Lemmatizing the word.\n",
    "                word = wordstem.stem(word)\n",
    "                tweetwords += (word+' ')\n",
    "            \n",
    "        processedText.append(tweetwords)\n",
    "        \n",
    "    return processedText, punctuation_counts\n",
    "\n",
    "print('Starting Function')\n",
    "processedText, punctuation_counts = %time preprocess(text)\n",
    "print('Ending Function')\n",
    "\n",
    "cleaned_text = pd.DataFrame(data=processedText, columns = ['tweet'])\n",
    "cleaned_text['punctuation_count'] = punctuation_counts\n",
    "cleaned_text['sentiment'] = sentiment\n",
    "\n",
    "#Created a .csv for review\n",
    "cleaned_text.to_csv('cleaned_text.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.18 s\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.07 TiB for an array with shape (105241, 1393267) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14552/741167571.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtfidf_feature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tfidf_vectorizer.get_feature_names_out()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtfidf_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtfidf_vect_w_mat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'column names started'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tfidf_df.columns = tfidf_feature_names # This may be a very expensive operation, consider commenting out'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36mtodense\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m         \"\"\"\n\u001b[1;32m--> 864\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.07 TiB for an array with shape (105241, 1393267) and data type float64"
     ]
    }
   ],
   "source": [
    "tfidf_feature_names = %time tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame (tfidf_vect_w_mat.todense())\n",
    "\n",
    "print('column names started')\n",
    "%time tfidf_df.columns = tfidf_feature_names # This may be a very expensive operation, consider commenting out\n",
    "print('column names completed')\n",
    "\n",
    "tfidf_df['punctuation_count'] = punctuation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Fc9tqVDMzfH4",
    "outputId": "958a0bc9-c8fb-469f-83d6-6c5075fe7c2e"
   },
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "data = data.drop(['ï»¿ItemID','SentimentSource','SentimentText','X_Clean'],axis =1)\n",
    "\n",
    "\n",
    "# create df that sentiment and clean text\n",
    "tfid_data = pd.concat([data,df_], axis=1)\n",
    "\n",
    "# Create a positive and Negative word Cloud\n",
    "\n",
    "# 1= postive \n",
    "neg_tweets = tfid_data[tfid_data.Sentiment == 1]\n",
    "neg_string = []\n",
    "for t in tfid_data.X_Clean:\n",
    "    neg_string.append(t)\n",
    "neg_string = pd.Series(neg_string).str.cat(sep=' ')\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neg_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# 0-=negative\n",
    "pos_tweets = tfid_data[tfid_data.Sentiment == 0]\n",
    "pos_string = []\n",
    "for t in tfid_data.X_Clean:\n",
    "    pos_string.append(t)\n",
    "pos_string = pd.Series(pos_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,colormap='magma').generate(pos_string) \n",
    "plt.figure(figsize=(12,10)) \n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
    "plt.axis(\"off\") \n",
    "plt.show()\n",
    "\n",
    "# Amount of words to be used in the TFIDF\n",
    "max_words = 2000\n",
    "\n",
    "\n",
    "#-- tf-idf --> Word level ----------------------------------------------------\n",
    "tfidf_vect_w = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                             max_features=max_words)\n",
    "\n",
    "tfidf_vect_w_mat = tfidf_vect_w.fit_transform(tfid_data.iloc[:,1])\n",
    "tfidf_w_names = tfidf_vect_w.get_feature_names()\n",
    "\n",
    "DF_tfidf_word = pd.DataFrame (tfidf_vect_w_mat.todense())\n",
    "\n",
    "\n",
    "#-- Assign the column names\n",
    "DF_tfidf_word.columns = tfidf_w_names\n",
    "\n",
    "\n",
    "# Prepare data for training\n",
    "\n",
    "X = np.asarray(DF_tfidf_word, dtype='float64') \n",
    "\n",
    "Y = tfid_data.iloc[:,0]\n",
    "ttrain_y = tf.keras.utils.to_categorical(Y, num_classes=2, dtype='float32')\n",
    "\n",
    "\n",
    "# Split the data into train and test data\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(X,ttrain_y,test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "# Define your Neural Network\n",
    "model = Sequential()\n",
    "model.add(Dense(550, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(225, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'])   \n",
    "    \n",
    "\n",
    "# Fit your model\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", monitor = 'val_acc', verbose=1, save_best_only=True) \n",
    "\n",
    "history = model.fit(\n",
    "                    train_x, \n",
    "                    train_y, \n",
    "                    epochs=5, \n",
    "                    batch_size=80, \n",
    "                    callbacks = [checkpointer], \n",
    "                    validation_data = (test_x, test_y)\n",
    "                    )\n",
    "\n",
    "\n",
    "# Save the model as twitter.h5\n",
    "model_name = \"twitter.h5\"\n",
    "model.save(model_name)\n",
    "model = load_model(model_name)\n",
    "\n",
    "y_pred = model.predict(test_x)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "test_y=np.argmax(test_y, axis =1)\n",
    "\n",
    "\n",
    "# 1= postive \n",
    "# 0-=negative\n",
    "labels = ['negative', 'positive']\n",
    "\n",
    "# Create a Confusion Matrix\n",
    "cm = confusion_matrix(y_pred, test_y)\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "\n",
    "# Plot your training and validation curves\n",
    "plt.subplots() # open a new plot\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()\n",
    "\n",
    "plt.subplots() # open a new plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "clean_tweet_dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

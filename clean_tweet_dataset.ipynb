{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YMDyA8VNL-3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72ShFB-4NFaU"
      },
      "outputs": [],
      "source": [
        "# This may require a runtime restart in order to work\n",
        "!pip install 'pandas==1.3.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CMXroCN0D35"
      },
      "outputs": [],
      "source": [
        "!pip install config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twij53zRzrrU"
      },
      "outputs": [],
      "source": [
        "!pip install 'tweepy==4.4.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "row3DN8VXxRB"
      },
      "outputs": [],
      "source": [
        "!pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgiRPiqx36lp",
        "outputId": "6d71ad03-ec72-4583-8d3d-f32ffe5e3aec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Standard Packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# NLTKPackages\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Nearal Network packages\n",
        "#import keras\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing, model_selection\n",
        "#from keras.models import Sequential, load_model \n",
        "#from keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Sklearn pacckages \n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN_Zd9tL1dXa"
      },
      "outputs": [],
      "source": [
        "#Loading the dataset for mac users\n",
        "FILE_PATH = '/content/drive/MyDrive/ML/GBC/DL1/Project/'\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Load the data set\n",
        "data = pd.read_csv(\n",
        "    FILE_PATH + 'sentiment_analysis_dataset.csv',\n",
        "    sep=',',\n",
        "    on_bad_lines='skip',\n",
        "    encoding='latin-1'\n",
        "    )\n",
        "\n",
        "print(data.shape)\n",
        "data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0RSOHBuS0wM",
        "outputId": "a024b42d-baab-4b1f-d06a-3e6757426262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1578612, 4)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ï»¿ItemID</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentSource</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>is so sad for my APL frie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>I missed the New Moon trail...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ï»¿ItemID  Sentiment SentimentSource  \\\n",
              "0          1          0    Sentiment140   \n",
              "1          2          0    Sentiment140   \n",
              "2          3          1    Sentiment140   \n",
              "3          4          0    Sentiment140   \n",
              "4          5          0    Sentiment140   \n",
              "\n",
              "                                       SentimentText  \n",
              "0                       is so sad for my APL frie...  \n",
              "1                     I missed the New Moon trail...  \n",
              "2                            omg its already 7:30 :O  \n",
              "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
              "4           i think mi bf is cheating on me!!!   ...  "
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading the dataset for PC users\n",
        "data = pd.read_csv('sentiment analysis dataset.csv', sep=',', on_bad_lines='skip', encoding='latin-1')\n",
        "print(data.shape)\n",
        "data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2R1emkARard",
        "outputId": "e423e974-d78c-4e83-f312-9d9ab7ef1515"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ï»¿ItemID          0\n",
              "Sentiment          0\n",
              "SentimentSource    0\n",
              "SentimentText      0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check for null data\n",
        "data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3so0G5USutZ",
        "outputId": "3d8e2361-0dbf-4826-8fb3-dccd6b4f17e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(105241, 3)"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The Item ID column is not useful for us, drop it\n",
        "# We also tested using 1/3 of the dataset and ran out of memory, 1/4 seems to be our limit\n",
        "data = data.drop(['ï»¿ItemID'], axis=1)\n",
        "data = data.sample(frac=1/15).reset_index(drop=True)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ2fpbBhS0wN",
        "outputId": "d9cff79e-0332-4d40-f91f-e937db0033b2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentSource</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>@joelMadden Im up too. I can't sleeeeeeeep! An...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>work at 8am..no fun</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>@PursuitBrooke Are you more of a Summer person...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>wants to smoke a cig but doesn't have a lighter.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Sentiment140</td>\n",
              "      <td>what pass?? there are lots of twits but we don...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentiment SentimentSource  \\\n",
              "0          0    Sentiment140   \n",
              "1          0    Sentiment140   \n",
              "2          1    Sentiment140   \n",
              "3          0    Sentiment140   \n",
              "4          0    Sentiment140   \n",
              "\n",
              "                                       SentimentText  \n",
              "0  @joelMadden Im up too. I can't sleeeeeeeep! An...  \n",
              "1                               work at 8am..no fun   \n",
              "2  @PursuitBrooke Are you more of a Summer person...  \n",
              "3  wants to smoke a cig but doesn't have a lighter.   \n",
              "4  what pass?? there are lots of twits but we don...  "
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaner \\# 1\n",
        "\n",
        "Using spell checking and lemmatizing. These are very slow functions, so can't be done to quickly process the whole dataset."
      ],
      "metadata": {
        "id": "TCyyuJi4S9LR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4txt18uS0wO",
        "outputId": "6728e8c2-5cbf-4094-a650-e9c09e1e4feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cleaned text started\n",
            "Wall time: 1min 53s\n",
            "cleaned text completed\n",
            "Wall time: 0 ns\n",
            "tfidf vectorizer started\n",
            "Wall time: 14 ms\n",
            "tfidf vectorizer completed\n",
            "Wall time: 1.02 ms\n",
            "column names started\n",
            "Wall time: 0 ns\n",
            "column names completed\n"
          ]
        }
      ],
      "source": [
        "# Clean text\n",
        "import re\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "def clean_text_for_tfidf_vectorizer(text):\n",
        "    spell = SpellChecker()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tk = TweetTokenizer()\n",
        "\n",
        "    stopword_list = stopwords.words('english')\n",
        "    new_stop_words=['i', 'im', 'http', 'ive', 'rt']\n",
        "    for i in new_stop_words:\n",
        "        stopword_list.append(i)\n",
        "\n",
        "    cleaned_text = []\n",
        "    punctuation_counts = []\n",
        "\n",
        "    for sentence in text:\n",
        "        cleaned_words = []\n",
        "\n",
        "        punctuation_count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
        "        punctuation_counts.append(punctuation_count(sentence,set(punctuation)))\n",
        "\n",
        "        for word in tk.tokenize(sentence):\n",
        "            # Spell check\n",
        "            word = spell.correction(word.lower())\n",
        "\n",
        "            # Remove stop words\n",
        "            if word in stopword_list:\n",
        "              continue\n",
        "\n",
        "            # Remove numbers and punctuation\n",
        "            word = re.sub('[^a-zA-Z]+', '', word)\n",
        "\n",
        "            if (word == ''):\n",
        "              continue\n",
        "\n",
        "            # Lemmatize\n",
        "            word = lemmatizer.lemmatize(word)\n",
        "\n",
        "            cleaned_words.append(word)\n",
        "\n",
        "        cleaned_text.append(' '.join(cleaned_words))\n",
        "      \n",
        "    return cleaned_text, punctuation_counts\n",
        "\n",
        "# Checking the run time of the script on a small subset of the dataset\n",
        "text = data['SentimentText']\n",
        "text = text.loc[1:500]\n",
        "\n",
        "print('cleaned text started')\n",
        "cleaned_text, punctuation_counts = %time clean_text_for_tfidf_vectorizer(text)\n",
        "print('cleaned text completed')\n",
        " \n",
        "tfidf_vectorizer = %time TfidfVectorizer(ngram_range=(1,3))\n",
        "\n",
        "print('tfidf vectorizer started')\n",
        "tfidf_vect_w_mat = %time tfidf_vectorizer.fit_transform(cleaned_text)\n",
        "print('tfidf vectorizer completed')\n",
        "\n",
        "tfidf_feature_names = %time tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_df = pd.DataFrame (tfidf_vect_w_mat.todense())\n",
        "\n",
        "print('column names started')\n",
        "%time tfidf_df.columns = tfidf_feature_names # This may be a very expensive operation, consider commenting out\n",
        "print('column names completed')\n",
        "\n",
        "tfidf_df['punctuation_count'] = punctuation_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRevNrOKS0wP"
      },
      "source": [
        "We tried this function to clean the dataset and benchmarked it's performance.  Only 500 tweets took almost 2 minutes, checked to see if we can reduce the time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaner \\# 2"
      ],
      "metadata": {
        "id": "DpRzeZiWTFH2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBSpqNtDS0wP"
      },
      "source": [
        "After testing, we discovered that the truly computationally expensive function was the spell checker.  We will be creating 2 versions of the database, one with spell checker and one without, so we can move forward with model building\n",
        "while the spell checker compiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbUx5IYzS0wP",
        "outputId": "ad575b71-f382-492e-ff2a-311911786d97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Function\n",
            "Wall time: 38min 35s\n",
            "Ending Function\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "stopwordlist = stopwords.words('english')\n",
        "new_stop_words=['i', 'im', 'http', 'ive', 'rt']\n",
        "for i in new_stop_words:\n",
        "    stopwordlist.append(i)\n",
        "\n",
        "\n",
        "text = data['SentimentText']\n",
        "sentiment = data['Sentiment']\n",
        "\n",
        "\n",
        "def preprocess(textdata):\n",
        "    processedText = []\n",
        "    punctuation_counts = []\n",
        "\n",
        "    # Create Lemmatizer and Stemmer.\n",
        "    wordstem = PorterStemmer()\n",
        "    spell = SpellChecker()\n",
        "    tk = TweetTokenizer()\n",
        "    \n",
        "    for tweet in textdata:\n",
        "        tweet = re.sub(r'@([A-Za-z0-9_]+)', '', tweet)\n",
        "        punctuation_count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
        "        punctuation_counts.append(punctuation_count(tweet,set(punctuation)))\n",
        "        tweet = tweet.lower()\n",
        "        \n",
        "        # Regex\n",
        "        tweet = re.sub('[^a-zA-Z]+', ' ', tweet)\n",
        "        sequencePattern   = r\"(.)\\1\\1+\"\n",
        "        seqReplacePattern = r\"\\1\\1\"\n",
        "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
        "        \n",
        "        tweetwords = ''\n",
        "        for word in tk.tokenize(tweet):\n",
        "            # Checking if the word is a stopword.\n",
        "            if word not in stopwordlist:\n",
        "                # Spell check the word\n",
        "                word = spell.correction(word)\n",
        "                # Lemmatizing the word.\n",
        "                word = wordstem.stem(word)\n",
        "                tweetwords += (word+' ')\n",
        "            \n",
        "        processedText.append(tweetwords)\n",
        "        \n",
        "    return processedText, punctuation_counts\n",
        "\n",
        "print('Starting Function')\n",
        "processedText, punctuation_counts = %time preprocess(text)\n",
        "print('Ending Function')\n",
        "\n",
        "cleaned_text = pd.DataFrame(data=processedText, columns = ['tweet'])\n",
        "cleaned_text['punctuation_count'] = punctuation_counts\n",
        "cleaned_text['sentiment'] = sentiment\n",
        "\n",
        "#Created a .csv for review\n",
        "cleaned_text.to_csv('cleaned_text.csv')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "clean_tweet_dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
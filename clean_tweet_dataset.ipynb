{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YMDyA8VNL-3"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72ShFB-4NFaU",
    "outputId": "a09eee88-5661-464a-f0c8-1db61314185c"
   },
   "outputs": [],
   "source": [
    "# This may require a runtime restart in order to work\n",
    "!pip install 'pandas==1.3.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2CMXroCN0D35",
    "outputId": "3d62460d-6275-4c1c-fd78-7e0956ab5843"
   },
   "outputs": [],
   "source": [
    "!pip install config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twij53zRzrrU",
    "outputId": "cc3f67ea-40b6-44b7-afd9-658dc425d7e5"
   },
   "outputs": [],
   "source": [
    "!pip install 'tweepy==4.4.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "row3DN8VXxRB",
    "outputId": "e613b5b0-20c0-4f66-b4e3-b38f5c4a1811"
   },
   "outputs": [],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgiRPiqx36lp",
    "outputId": "6d71ad03-ec72-4583-8d3d-f32ffe5e3aec"
   },
   "outputs": [],
   "source": [
    "# Standard Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLTKPackages\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Nearal Network packages\n",
    "#import keras\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing, model_selection\n",
    "#from keras.models import Sequential, load_model \n",
    "#from keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Sklearn pacckages \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "PN_Zd9tL1dXa",
    "outputId": "9c3c267f-fc56-4c2f-9307-b594cfd1bcd7"
   },
   "outputs": [],
   "source": [
    "#Loading the dataset for mac users\n",
    "FILE_PATH = '/content/drive/MyDrive/ML/GBC/DL1/Project/'\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Load the data set\n",
    "data = pd.read_csv(\n",
    "    FILE_PATH + 'sentiment_analysis_dataset.csv',\n",
    "    sep=',',\n",
    "    on_bad_lines='skip',\n",
    "    encoding='latin-1'\n",
    "    )\n",
    "\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1578612, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ï»¿ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ï»¿ItemID  Sentiment SentimentSource  \\\n",
       "0          1          0    Sentiment140   \n",
       "1          2          0    Sentiment140   \n",
       "2          3          1    Sentiment140   \n",
       "3          4          0    Sentiment140   \n",
       "4          5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset for PC users\n",
    "data = pd.read_csv('sentiment analysis dataset.csv', sep=',', on_bad_lines='skip', encoding='latin-1')\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L2R1emkARard",
    "outputId": "e423e974-d78c-4e83-f312-9d9ab7ef1515"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ï»¿ItemID          0\n",
       "Sentiment          0\n",
       "SentimentSource    0\n",
       "SentimentText      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null data\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "E3so0G5USutZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105241, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Item ID column is not useful for us, drop it\n",
    "# We also tested using 1/3 of the dataset and ran out of memory, 1/4 seems to be our limit\n",
    "data = data.drop(['ï»¿ItemID'], axis=1)\n",
    "data = data.sample(frac=1/15).reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UYa_ZgBvLRsJ"
   },
   "outputs": [],
   "source": [
    "# Clean text\n",
    "import re\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def clean_text_for_tfidf_vectorizer(text):\n",
    "    spell = SpellChecker()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tk = TweetTokenizer()\n",
    "\n",
    "    stopword_list = stopwords.words('english')\n",
    "    new_stop_words=['i', 'im', 'http', 'ive', 'rt']\n",
    "    for i in new_stop_words:\n",
    "        stopword_list.append(i)\n",
    "\n",
    "    cleaned_text = []\n",
    "    punctuation_counts = []\n",
    "\n",
    "    for sentence in text:\n",
    "        cleaned_words = []\n",
    "\n",
    "        punctuation_count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "        punctuation_counts.append(punctuation_count(sentence,set(punctuation)))\n",
    "\n",
    "        for word in tk.tokenize(sentence):\n",
    "            # Spell check\n",
    "            word = spell.correction(word.lower())\n",
    "\n",
    "            # Remove stop words\n",
    "            if word in stopword_list:\n",
    "              continue\n",
    "\n",
    "            # Remove numbers and punctuation\n",
    "            word = re.sub('[^a-zA-Z]+', '', word)\n",
    "\n",
    "            if (word == ''):\n",
    "              continue\n",
    "\n",
    "            # Lemmatize\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "\n",
    "            cleaned_words.append(word)\n",
    "\n",
    "        cleaned_text.append(' '.join(cleaned_words))\n",
    "      \n",
    "    return cleaned_text, punctuation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "GZI4xSwcM0zL",
    "outputId": "769796e5-b59c-40a9-9855-18ee23c3df4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentences:\n",
      "['Hello, my name is Daniel.', 'This is my example sentence!', 'I am exxaggerating and have mispelled a word!!!!!']\n",
      "\n",
      "Cleaned sentences:\n",
      "['hello name daniel', 'example sentence', 'exaggerating misspelled word']\n",
      "\n",
      "tfidf feature names:\n",
      "['daniel' 'exaggerating' 'exaggerating misspelled'\n",
      " 'exaggerating misspelled word' 'example' 'example sentence' 'hello'\n",
      " 'hello name' 'hello name daniel' 'misspelled' 'misspelled word' 'name'\n",
      " 'name daniel' 'sentence' 'word']\n",
      "\n",
      "tfidf df with punctuation count:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daniel</th>\n",
       "      <th>exaggerating</th>\n",
       "      <th>exaggerating misspelled</th>\n",
       "      <th>exaggerating misspelled word</th>\n",
       "      <th>example</th>\n",
       "      <th>example sentence</th>\n",
       "      <th>hello</th>\n",
       "      <th>hello name</th>\n",
       "      <th>hello name daniel</th>\n",
       "      <th>misspelled</th>\n",
       "      <th>misspelled word</th>\n",
       "      <th>name</th>\n",
       "      <th>name daniel</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>punctuation_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     daniel  exaggerating  exaggerating misspelled  \\\n",
       "0  0.408248      0.000000                 0.000000   \n",
       "1  0.000000      0.000000                 0.000000   \n",
       "2  0.000000      0.408248                 0.408248   \n",
       "\n",
       "   exaggerating misspelled word  example  example sentence     hello  \\\n",
       "0                      0.000000  0.00000           0.00000  0.408248   \n",
       "1                      0.000000  0.57735           0.57735  0.000000   \n",
       "2                      0.408248  0.00000           0.00000  0.000000   \n",
       "\n",
       "   hello name  hello name daniel  misspelled  misspelled word      name  \\\n",
       "0    0.408248           0.408248    0.000000         0.000000  0.408248   \n",
       "1    0.000000           0.000000    0.000000         0.000000  0.000000   \n",
       "2    0.000000           0.000000    0.408248         0.408248  0.000000   \n",
       "\n",
       "   name daniel  sentence      word  punctuation_count  \n",
       "0     0.408248   0.00000  0.000000                  2  \n",
       "1     0.000000   0.57735  0.000000                  1  \n",
       "2     0.000000   0.00000  0.408248                  5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of cleaning code in action\n",
    "text = ['Hello, my name is Daniel.', 'This is my example sentence!', 'I am exxaggerating and have mispelled a word!!!!!']\n",
    "cleaned_text, punctuation_counts = clean_text_for_tfidf_vectorizer(text)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "tfidf_vect_w_mat = tfidf_vectorizer.fit_transform(cleaned_text)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame (tfidf_vect_w_mat.todense())\n",
    "tfidf_df.columns = tfidf_feature_names # This may be a very expensive operation, consider commenting out\n",
    "tfidf_df['punctuation_count'] = punctuation_counts\n",
    "print('Original sentences:')\n",
    "print(text)\n",
    "print()\n",
    "print('Cleaned sentences:')\n",
    "print(cleaned_text)\n",
    "print()\n",
    "print('tfidf feature names:')\n",
    "print(tfidf_feature_names)\n",
    "print()\n",
    "print('tfidf df with punctuation count:')\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned text started\n",
      "Wall time: 1min 53s\n",
      "cleaned text completed\n",
      "Wall time: 0 ns\n",
      "tfidf vectorizer started\n",
      "Wall time: 14 ms\n",
      "tfidf vectorizer completed\n",
      "Wall time: 1.02 ms\n",
      "column names started\n",
      "Wall time: 0 ns\n",
      "column names completed\n"
     ]
    }
   ],
   "source": [
    "# Checking the run time of the script on a small subset of the dataset\n",
    "text = data['SentimentText']\n",
    "text = text.loc[1:500]\n",
    "\n",
    "print('cleaned text started')\n",
    "cleaned_text, punctuation_counts = %time clean_text_for_tfidf_vectorizer(text)\n",
    "print('cleaned text completed')\n",
    " \n",
    "tfidf_vectorizer = %time TfidfVectorizer(ngram_range=(1,3))\n",
    "\n",
    "print('tfidf vectorizer started')\n",
    "tfidf_vect_w_mat = %time tfidf_vectorizer.fit_transform(cleaned_text)\n",
    "print('tfidf vectorizer completed')\n",
    "\n",
    "tfidf_feature_names = %time tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame (tfidf_vect_w_mat.todense())\n",
    "\n",
    "print('column names started')\n",
    "%time tfidf_df.columns = tfidf_feature_names # This may be a very expensive operation, consider commenting out\n",
    "print('column names completed')\n",
    "\n",
    "tfidf_df['punctuation_count'] = punctuation_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried this function to clean the dataset and benchmarked it's performance.  Only 500 tweets took almost 2 minutes, checked to see if we can reduce the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Function\n",
      "Wall time: 57.3 s\n",
      "Ending Function\n",
      "Wall time: 0 ns\n",
      "tfidf vectorizer started\n",
      "Wall time: 27.2 ms\n",
      "tfidf vectorizer completed\n",
      "Wall time: 2.99 ms\n",
      "column names started\n",
      "Wall time: 997 µs\n",
      "column names completed\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "text = data['SentimentText']\n",
    "text = text.loc[1:500]\n",
    "processedText = []\n",
    "punctuation_counts = []\n",
    "\n",
    "def preprocess(textdata):\n",
    "    \n",
    "    # Create Lemmatizer and Stemmer.\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    spell = SpellChecker()\n",
    "    \n",
    "    for tweet in textdata:\n",
    "        punctuation_count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "        punctuation_counts.append(punctuation_count(tweet,set(punctuation)))\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        # Replace all URls with 'URL'\n",
    "        tweet = re.sub('[^a-zA-Z]+', ' ', tweet)      \n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "            # Checking if the word is a stopword.\n",
    "            #if word not in stopwordlist:\n",
    "            if len(word)>1:\n",
    "                # Spell check the word\n",
    "                word = spell.correction(word)\n",
    "                # Lemmatizing the word.\n",
    "                word = wordLemm.lemmatize(word)\n",
    "                tweetwords += (word+' ')\n",
    "            \n",
    "        processedText.append(tweetwords)\n",
    "        \n",
    "    return processedText\n",
    "\n",
    "print('Starting Function')\n",
    "%time preprocess(text)\n",
    "print('Ending Function')\n",
    "\n",
    "tfidf_vectorizer = %time TfidfVectorizer(ngram_range=(1,3))\n",
    "\n",
    "print('tfidf vectorizer started')\n",
    "tfidf_vect_w_mat = %time tfidf_vectorizer.fit_transform(processedText)\n",
    "print('tfidf vectorizer completed')\n",
    "\n",
    "tfidf_feature_names = %time tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame (tfidf_vect_w_mat.todense())\n",
    "\n",
    "print('column names started')\n",
    "%time tfidf_df.columns = tfidf_feature_names # This may be a very expensive operation, consider commenting out\n",
    "print('column names completed')\n",
    "\n",
    "tfidf_df['punctuation_count'] = punctuation_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After various testing, the expensive function is the spell checker.  We will be creating 2 versions of the database, one with spell checker and one without so we can move forward with model building\n",
    "while the spell checker compiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Function\n",
      "Wall time: 3h 5min 11s\n",
      "Ending Function\n",
      "Wall time: 0 ns\n",
      "tfidf vectorizer started\n",
      "Wall time: 5.97 s\n",
      "tfidf vectorizer completed\n"
     ]
    }
   ],
   "source": [
    "text = data['SentimentText']\n",
    "processedText = []\n",
    "punctuation_counts = []\n",
    "\n",
    "\n",
    "def preprocess(textdata):\n",
    "\n",
    "    # Create Lemmatizer and Stemmer.\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    spell = SpellChecker()\n",
    "    \n",
    "    for tweet in textdata:\n",
    "        punctuation_count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "        punctuation_counts.append(punctuation_count(tweet,set(punctuation)))\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        # Replace all URls with 'URL'\n",
    "        tweet = re.sub('[^a-zA-Z]+', ' ', tweet)\n",
    "        sequencePattern   = r\"(.)\\1\\1+\"\n",
    "        seqReplacePattern = r\"\\1\\1\"\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "        \n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "            # Checking if the word is a stopword.\n",
    "            #if word not in stopwordlist:\n",
    "            if len(word)>1:\n",
    "                # Spell check the word\n",
    "                word = spell.correction(word)\n",
    "                # Lemmatizing the word.\n",
    "                word = wordLemm.lemmatize(word)\n",
    "                tweetwords += (word+' ')\n",
    "            \n",
    "        processedText.append(tweetwords)\n",
    "        \n",
    "    return processedText\n",
    "\n",
    "print('Starting Function')\n",
    "%time preprocess(text)\n",
    "print('Ending Function')\n",
    "\n",
    "tfidf_vectorizer = %time TfidfVectorizer(ngram_range=(1,3))\n",
    "\n",
    "print('tfidf vectorizer started')\n",
    "tfidf_vect_w_mat = %time tfidf_vectorizer.fit_transform(processedText)\n",
    "print('tfidf vectorizer completed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<31572x507414 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1062004 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect_w_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = pd.DataFrame(processedText, columns = ['column_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kenichan ugh hate that quit foot in agony quit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pandi no no em do no no no utweetme cho ti he he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wish it wa time to get train home today seems ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at work forgot my lunch have to queue up with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>janeejohnson if all say so trust you on that one</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         column_name\n",
       "0  kenichan ugh hate that quit foot in agony quit...\n",
       "1  pandi no no em do no no no utweetme cho ti he he \n",
       "2  wish it wa time to get train home today seems ...\n",
       "3  at work forgot my lunch have to queue up with ...\n",
       "4  janeejohnson if all say so trust you on that one "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text['punctuation_count'] = punctuation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a .csv for review\n",
    "cleaned_text.to_csv('cleaned_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.18 s\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.07 TiB for an array with shape (105241, 1393267) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14552/741167571.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtfidf_feature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tfidf_vectorizer.get_feature_names_out()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtfidf_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtfidf_vect_w_mat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'column names started'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tfidf_df.columns = tfidf_feature_names # This may be a very expensive operation, consider commenting out'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36mtodense\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m         \"\"\"\n\u001b[1;32m--> 864\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.07 TiB for an array with shape (105241, 1393267) and data type float64"
     ]
    }
   ],
   "source": [
    "tfidf_feature_names = %time tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame (tfidf_vect_w_mat.todense())\n",
    "\n",
    "print('column names started')\n",
    "%time tfidf_df.columns = tfidf_feature_names # This may be a very expensive operation, consider commenting out\n",
    "print('column names completed')\n",
    "\n",
    "tfidf_df['punctuation_count'] = punctuation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Fc9tqVDMzfH4",
    "outputId": "958a0bc9-c8fb-469f-83d6-6c5075fe7c2e"
   },
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "data = data.drop(['ï»¿ItemID','SentimentSource','SentimentText','X_Clean'],axis =1)\n",
    "\n",
    "\n",
    "# create df that sentiment and clean text\n",
    "tfid_data = pd.concat([data,df_], axis=1)\n",
    "\n",
    "# Create a positive and Negative word Cloud\n",
    "\n",
    "# 1= postive \n",
    "neg_tweets = tfid_data[tfid_data.Sentiment == 1]\n",
    "neg_string = []\n",
    "for t in tfid_data.X_Clean:\n",
    "    neg_string.append(t)\n",
    "neg_string = pd.Series(neg_string).str.cat(sep=' ')\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neg_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# 0-=negative\n",
    "pos_tweets = tfid_data[tfid_data.Sentiment == 0]\n",
    "pos_string = []\n",
    "for t in tfid_data.X_Clean:\n",
    "    pos_string.append(t)\n",
    "pos_string = pd.Series(pos_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,colormap='magma').generate(pos_string) \n",
    "plt.figure(figsize=(12,10)) \n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
    "plt.axis(\"off\") \n",
    "plt.show()\n",
    "\n",
    "# Amount of words to be used in the TFIDF\n",
    "max_words = 2000\n",
    "\n",
    "\n",
    "#-- tf-idf --> Word level ----------------------------------------------------\n",
    "tfidf_vect_w = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                             max_features=max_words)\n",
    "\n",
    "tfidf_vect_w_mat = tfidf_vect_w.fit_transform(tfid_data.iloc[:,1])\n",
    "tfidf_w_names = tfidf_vect_w.get_feature_names()\n",
    "\n",
    "DF_tfidf_word = pd.DataFrame (tfidf_vect_w_mat.todense())\n",
    "\n",
    "\n",
    "#-- Assign the column names\n",
    "DF_tfidf_word.columns = tfidf_w_names\n",
    "\n",
    "\n",
    "# Prepare data for training\n",
    "\n",
    "X = np.asarray(DF_tfidf_word, dtype='float64') \n",
    "\n",
    "Y = tfid_data.iloc[:,0]\n",
    "ttrain_y = tf.keras.utils.to_categorical(Y, num_classes=2, dtype='float32')\n",
    "\n",
    "\n",
    "# Split the data into train and test data\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(X,ttrain_y,test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "# Define your Neural Network\n",
    "model = Sequential()\n",
    "model.add(Dense(550, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(225, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'])   \n",
    "    \n",
    "\n",
    "# Fit your model\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", monitor = 'val_acc', verbose=1, save_best_only=True) \n",
    "\n",
    "history = model.fit(\n",
    "                    train_x, \n",
    "                    train_y, \n",
    "                    epochs=5, \n",
    "                    batch_size=80, \n",
    "                    callbacks = [checkpointer], \n",
    "                    validation_data = (test_x, test_y)\n",
    "                    )\n",
    "\n",
    "\n",
    "# Save the model as twitter.h5\n",
    "model_name = \"twitter.h5\"\n",
    "model.save(model_name)\n",
    "model = load_model(model_name)\n",
    "\n",
    "y_pred = model.predict(test_x)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "test_y=np.argmax(test_y, axis =1)\n",
    "\n",
    "\n",
    "# 1= postive \n",
    "# 0-=negative\n",
    "labels = ['negative', 'positive']\n",
    "\n",
    "# Create a Confusion Matrix\n",
    "cm = confusion_matrix(y_pred, test_y)\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "\n",
    "# Plot your training and validation curves\n",
    "plt.subplots() # open a new plot\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()\n",
    "\n",
    "plt.subplots() # open a new plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "clean_tweet_dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
